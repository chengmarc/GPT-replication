[![conda - v23.7.3](https://img.shields.io/static/v1?label=conda&message=v23.7.3&color=green&logo=anaconda&logoColor=white)](https://)
[![python - v3.10.11](https://img.shields.io/static/v1?label=python&message=v3.10.11&color=blue&logo=python&logoColor=white)](https://)
[![torch - v2.0.1](https://img.shields.io/static/v1?label=torch&message=v2.0.1&color=orange&logo=pytorch&logoColor=white)](https://)

### Repository Summary ###
- Task: Natural Language Processing
- Model Type: Generative Pre-training Transformer
- Total number of parameters: 162,419,712
- Total size of the model: 619.58 MB

### Theoretic References ###

[1] Language Models are Unsupervised Multitask Learners

    Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever.
    https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

[2] Attention Is All You Need

    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.
    https://arxiv.org/pdf/1706.03762
    
[3] Distributed Representations of Words and Phrases and their Compositionality

    Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.
    https://arxiv.org/abs/1310.4546
    
[4] Gradient-based learning applied to document recognition

    Y. Lecun, L. Bottou, Y. Bengio, P. Haffner.
    http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
    
[5] Dropout: A Simple Way to Prevent Neural Networks from Overfitting

    Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov.
    https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
    
[6] Layer Normalization

    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton.
    https://arxiv.org/pdf/1607.06450
    
[7] Gaussian Error Linear Units (GELUs)

    Dan Hendrycks, Kevin Gimpel.
    https://arxiv.org/pdf/1606.08415
    
[8] Deep Learning using Rectified Linear Units (ReLU)

    Abien Fred Agarap.
    https://arxiv.org/abs/1803.08375
    
[9] A new algorithm for data compression (Byte Pair Encoding)

    Philip Gage.
    https://dl.acm.org/doi/abs/10.5555/177910.177914

### Dataset Reference ###
[10] Harry Potter all books (preprocessed)

    Mateusz Kud≈Ça
    https://www.kaggle.com/datasets/moxxis/harry-potter-lstm
    
### Code Reference ### 
[11] Build a Large Language Model (From Scratch)

    Sebastian Raschka 
    https://github.com/rasbt/LLMs-from-scratch
